{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** *Your group letter.*\n",
    "\n",
    "**Names:**\n",
    "\n",
    "* *Guillem Pruñonosa Soler*\n",
    "* *Name 2*\n",
    "* *Name 3*\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/prunonos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, linalg\n",
    "\n",
    "from utils import load_json, load_pkl\n",
    "# imported libraries\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('wordnet') # lemmatization\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This course provides students with the framework and decision tools needed for taking financial decisions and evaluating investment opportunities in a global economy. We use an integrated model of exchange rate and output determination to analyze the effects of monetary and fiscal policies. Content National Income Accounting and the Balance of Payments Exchange Rates and the Foreign Exchange Market: An Asset Approach Money, Interest Rates and Exchange Rates; Price Level and the Exchange Rate in the Long Run Output and the Exchange Rate in the Short Run with Fixed and Flexible Exchange Rates Fixed Exchange Rates and the Dynamics of Currency Crises Financial Crises and the Choice of Exchange Rate Regime Currency Unions and the European Experience The Global Capital Market Keywords International finance, open-economy macroeconomics Learning Outcomes By the end of the course, the student must be able to: Define the determinants of exchange rates in the short runIllustrate the role of money supply and interest rates in determining exchange rates in the long runDevelop an integrated model of output and exchange rate determinationIllustrate how monetary and fiscal policies affect output and exchange rates in the short and the long runExplain what is a common currency areaAssess / Evaluate the impact of forming a common currency area, like the EurozoneAssess / Evaluate how countries are interconnected via the current account and changes in net foreign wealthIntegrate the role of nontraded goods/factors in determining exchange rates in the long runCompare different exchange rate regimesExpound currency crisesIllustrate the link between financial crises and the choice of exchange-rate regime Transversal skills Evaluate one's own performance in the team, receive and respond appropriately to feedback.Give feedback (critique) in an appropriate fashion.Access and evaluate appropriate sources of information.Use a work methodology appropriate to the task. Teaching methods The course is organized in lectures, class discussions and presentation of case studies. Lectures will provide the theoretical knowledge needed to understand global economic events. Students will have the opportunity to test their knowledge by doing weekly problem sets, which will be graded. Lectures will include several class discussions of recent economic events, which will allow students to apply the theory learned in class. Students will be divided in groups to work on a case study, which they will present in class. The topic of the case study varies from year to year and it will be chosen a few weeks into the course. The case study and its presentation will be part of the final grade. Expected student activities PROBLEM SETS Problem sets will begiven out each Monday and need to be returned to the instructor at the beginning of class the following Monday. The solution to the problem set will be available on the class web page. No late problem set will be accepted. All questions in the problem set will be graded. While working with other students on problem sets is allowed, each student must complete his/her problem set. I strongly recommend you to do the problem sets: this is the best way to learn the material and do well in the course. GROUP CASE STUDY Students will be divided in groups. Each group will prepare and present a case study that is typically related to current economic events. For example, last year the main topic was the Eurozone crisis and each group was assigned a specific topic related to it. The topic of your case study will be announced a few weeks into the course. The case studies will be presented in class and followed by general discussion. You will be graded on the quality of the material, the presentation, and the contribution to the class discussion of other groups' case studies. Assessment methods 30 % Problem sets, class participation and case study 35 % Midterm Exam (closed book) 35 % Final Exam (closed book) Supervision Office hours Yes Assistants Yes Forum No\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = courses[2].get('description')\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'actually', 'was', 'better', 'how']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stopwords)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prunonos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "example_tokenized = nltk.tokenize.word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_lowercase = [word.lower() for word in example_tokenized if (word.isalpha() and word not in stopwords)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try first with **stemming**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "example_stemmed = [ps.stem(word.lower()) for word in example_tokenized if (word.isalpha() and word.lower() not in stopwords)]\n",
    "len(example_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.stem.WordNetLemmatizer()\n",
    "example_lemma = [lm.lemmatize(word.lower()) for word in example_tokenized if (word.isalpha() and word.lower() not in stopwords)]\n",
    "len(example_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print some examples, in order to show the difference between the stemming and lemmatization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming \t lemmatization\n",
      "------------------------------\n",
      "student     \t student\n",
      "opportun     \t opportunity\n",
      "effect     \t effect\n",
      "exchang     \t exchange\n",
      "exchang     \t exchange\n",
      "rate     \t rate\n",
      "dynam     \t dynamic\n",
      "union     \t union\n",
      "learn     \t learning\n",
      "role     \t role\n",
      "integr     \t integrated\n",
      "output     \t output\n",
      "impact     \t impact\n",
      "account     \t account\n",
      "runcompar     \t runcompare\n",
      "regim     \t regime\n",
      "critiqu     \t critique\n",
      "class     \t class\n",
      "understand     \t understand\n",
      "set     \t set\n",
      "appli     \t apply\n",
      "present     \t present\n",
      "case     \t case\n",
      "set     \t set\n",
      "solut     \t solution\n",
      "question     \t question\n",
      "complet     \t complete\n",
      "case     \t case\n",
      "typic     \t typically\n",
      "group     \t group\n",
      "case     \t case\n",
      "contribut     \t contribution\n",
      "class     \t class\n",
      "close     \t closed\n"
     ]
    }
   ],
   "source": [
    "print('stemming \\t lemmatization')\n",
    "print('------------------------------')\n",
    "for i,(s,l) in enumerate(zip(example_stemmed,example_lemma)):\n",
    "    if i%10==0: print(s,'    \\t',l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we chose lemmatization, we create the ngrams in order to represent better the meaning of the text.\n",
    "In this case, we chose ngrams with n=3. But the *n* is a hyperparameter we want to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['student framework decision',\n",
       " 'framework decision tool',\n",
       " 'decision tool needed',\n",
       " 'tool needed taking',\n",
       " 'needed taking financial',\n",
       " 'taking financial decision',\n",
       " 'financial decision evaluating',\n",
       " 'decision evaluating investment',\n",
       " 'evaluating investment opportunity',\n",
       " 'investment opportunity global',\n",
       " 'opportunity global economy',\n",
       " 'global economy integrated',\n",
       " 'economy integrated model',\n",
       " 'integrated model exchange',\n",
       " 'model exchange rate',\n",
       " 'exchange rate output',\n",
       " 'rate output determination',\n",
       " 'output determination analyze',\n",
       " 'determination analyze effect',\n",
       " 'analyze effect monetary']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = nltk.ngrams(example_lemma, 3)\n",
    "example_ngram3 = []\n",
    "for n3 in ngrams:\n",
    "    n3 = \" \".join(n3) # we join each 3-gram into a single string. \n",
    "    example_ngram3.append(n3)\n",
    "    \n",
    "print(len(example_ngram3))\n",
    "example_ngram3[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check which are the most and less common words in our corpus. We will train a model keeping them and another removing them, in order to see in which case our model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountWords(dict_,corpus):\n",
    "    for word in corpus:\n",
    "        dict_[word] += 1\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = defaultdict(int)\n",
    "for course in courses: \n",
    "    course_token = nltk.tokenize.word_tokenize(course.get('description'))\n",
    "#     course_lemma = [lm.lemmatize(word.lower()) for word in course_token if (word.isalpha() and word.lower() not in stopwords)]\n",
    "    for word in course_token:\n",
    "        word = word.lower()\n",
    "        if (word.isalpha() and word not in stopwords):\n",
    "#             word_lemma = lm.lemmatize(word)\n",
    "            word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count\n",
    "word_count_values             =   np.array(list(word_count.values()))\n",
    "word_count_keys               =   np.array(list(word_count.keys()))\n",
    "\n",
    "idxs_sorted                   =   np.argsort(word_count_values)[::-1]\n",
    "sorted_word_count_values      =   word_count_values[idxs_sorted]\n",
    "sorted_word_count_keys        =   word_count_keys[idxs_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most and less words in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in our corpus:\n",
      "\n",
      "methods 1617\n",
      "learning 1470\n",
      "student 1210\n",
      "content 904\n",
      "students 833\n",
      "courses 758\n",
      "design 751\n",
      "systems 722\n",
      "analysis 704\n",
      "end 666\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Less common words in our corpus:\n",
      "\n",
      "pin 1\n",
      "novissima 1\n",
      "productslist 1\n",
      "involvedpresent 1\n",
      "nanosystemsapply 1\n",
      "nanosystemsdescribe 1\n",
      "tsvs 1\n",
      "hermetic 1\n",
      "cite 1\n"
     ]
    }
   ],
   "source": [
    "for i,(w,c) in enumerate(zip(sorted_word_count_keys, sorted_word_count_values)):\n",
    "    if i==0: print('Most common words in our corpus:\\n')\n",
    "    if i < 10:\n",
    "        print(w,c)\n",
    "\n",
    "    if i==11: print('\\n-------------------------------\\n\\nLess common words in our corpus:\\n')\n",
    "    if i > (len(idxs_sorted)-10):\n",
    "        print(w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 13982 words in our corpus\n"
     ]
    }
   ],
   "source": [
    "print(f'We have a total of {len(word_count_values)} words in our corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter to remove the words that appear only once in the whole corpus, and also the 20 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words we are going to remove: 7016\n",
      "We remove ~50% of all the words.\n"
     ]
    }
   ],
   "source": [
    "mask = (sorted_word_count_values <= 1) # we filter the sorted word count values array\n",
    "mask[:20] = True # we also remove the top 20 most common words of our corpus\n",
    "print(f'Number of words we are going to remove: {sum(mask)}')\n",
    "print(f'We remove ~{int(sum(mask)/len(word_count_values)*100)}% of all the words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_values             =   sorted_word_count_values[mask]\n",
    "commonwords                   =   sorted_word_count_keys[mask]\n",
    "# idxs_sorted                   =   np.argsort(word_count_values)[::-1]\n",
    "# sorted_words_frequency        =   word_count.keys()[idxs_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['methods', 'learning', 'student', ..., 'tsvs', 'hermetic', 'cite'],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commonwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we merge everything we have done until now in order to compute our bag-of-words of 3-gram after lemmatization and removing the stopwords, and the most and less common words of our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters if it's a stopword, or a very or nothing common word. Also checks if its a word or not.\n",
    "def filtering_words(w):\n",
    "    w = w.lower()\n",
    "    return (w.isalpha() and (w not in stopwords) and (w not in commonwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/prunonos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "n = 2       # ngrams\n",
    "lm = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "words_doc = []\n",
    "words = []\n",
    "# word_count = defaultdict(int)\n",
    "\n",
    "for course in courses:\n",
    "    desc_proc = []\n",
    "    \n",
    "    desc_tokenized = nltk.tokenize.word_tokenize(course.get('description'))\n",
    "    desc_lemma     = [lm.lemmatize(word.lower()) for word in desc_tokenized if filtering_words(word)]\n",
    "        \n",
    "    ngrams = map(lambda x: ' '.join(x) , nltk.ngrams(desc_lemma, n))\n",
    "    for n3 in ngrams:\n",
    "        desc_proc.append(n3)\n",
    "        words.append(n3)\n",
    "        \n",
    "    desc_proc = sorted(desc_proc)\n",
    "                 \n",
    "    words_doc.append(desc_proc)\n",
    "\n",
    "words = np.unique(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have decided not to remove the most and less common words of our corpus, because we think they can be important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following output is the 2-gram representation of the course *Internet Analytics*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acquired lecture',\n",
       " 'activity lecture',\n",
       " 'ad auction',\n",
       " 'ad auction',\n",
       " 'advertisement class',\n",
       " 'algebra algorithm',\n",
       " 'algebra markov',\n",
       " 'algorithm data',\n",
       " 'algorithm statistic',\n",
       " 'analytics application',\n",
       " 'analytics collection',\n",
       " 'application inspired',\n",
       " 'application social',\n",
       " 'auction provide',\n",
       " 'auction required',\n",
       " 'balance foundational',\n",
       " 'based hadoop',\n",
       " 'based number',\n",
       " 'cathedra homework',\n",
       " 'chain java',\n",
       " 'class balance',\n",
       " 'class explores',\n",
       " 'class lab',\n",
       " 'cloud service',\n",
       " 'clustering community',\n",
       " 'clustering community',\n",
       " 'collection modeling',\n",
       " 'combination theoretical',\n",
       " 'communication recommended',\n",
       " 'community detection',\n",
       " 'community detection',\n",
       " 'computing ad',\n",
       " 'computing online',\n",
       " 'concrete problem',\n",
       " 'coverage main',\n",
       " 'current practice',\n",
       " 'data mining',\n",
       " 'data mining',\n",
       " 'data mining',\n",
       " 'data online',\n",
       " 'data online',\n",
       " 'data structure',\n",
       " 'datasets class',\n",
       " 'datasets real',\n",
       " 'decade class',\n",
       " 'dedicated infrastructure',\n",
       " 'designed explore',\n",
       " 'detection model',\n",
       " 'detection topic',\n",
       " 'dimensionality reduction',\n",
       " 'draw knowledge',\n",
       " 'effectiveness machine',\n",
       " 'efficiency effectiveness',\n",
       " 'expected activity',\n",
       " 'explore data',\n",
       " 'explore datasets',\n",
       " 'explore model',\n",
       " 'explore practical',\n",
       " 'explores number',\n",
       " 'field application',\n",
       " 'final exam',\n",
       " 'foundational material',\n",
       " 'framework model',\n",
       " 'function online',\n",
       " 'fundamental lab',\n",
       " 'good coverage',\n",
       " 'graph linear',\n",
       " 'graph theory',\n",
       " 'hadoop recommender',\n",
       " 'hadoop spark',\n",
       " 'homework explore',\n",
       " 'homework lab',\n",
       " 'important start',\n",
       " 'information network',\n",
       " 'information retrieval',\n",
       " 'infrastructure based',\n",
       " 'inspired current',\n",
       " 'internet analytics',\n",
       " 'internet cloud',\n",
       " 'java explore',\n",
       " 'key function',\n",
       " 'knowledge acquired',\n",
       " 'lab designed',\n",
       " 'lab draw',\n",
       " 'lab session',\n",
       " 'laboratory session',\n",
       " 'lecture homework',\n",
       " 'lecture midterm',\n",
       " 'linear algebra',\n",
       " 'linear algebra',\n",
       " 'machine social',\n",
       " 'machine technique',\n",
       " 'main data',\n",
       " 'markov chain',\n",
       " 'material algorithm',\n",
       " 'material weekly',\n",
       " 'medium combination',\n",
       " 'midterm final',\n",
       " 'mining analytics',\n",
       " 'mining machine',\n",
       " 'mining problem',\n",
       " 'model communication',\n",
       " 'model dimensionality',\n",
       " 'model fundamental',\n",
       " 'model information',\n",
       " 'model typical',\n",
       " 'modeling user',\n",
       " 'network recommender',\n",
       " 'networking hadoop',\n",
       " 'networking search',\n",
       " 'networking social',\n",
       " 'number datasets',\n",
       " 'number key',\n",
       " 'online ad',\n",
       " 'online efficiency',\n",
       " 'online framework',\n",
       " 'online service',\n",
       " 'online service',\n",
       " 'past decade',\n",
       " 'practical question',\n",
       " 'practice internet',\n",
       " 'problem cathedra',\n",
       " 'problem online',\n",
       " 'provide good',\n",
       " 'question based',\n",
       " 'real world',\n",
       " 'recommended linear',\n",
       " 'recommender clustering',\n",
       " 'recommender clustering',\n",
       " 'reduction stream',\n",
       " 'related field',\n",
       " 'required stochastic',\n",
       " 'retrieval stream',\n",
       " 'search advertisement',\n",
       " 'service social',\n",
       " 'service specifically',\n",
       " 'service ubiquitous',\n",
       " 'session expected',\n",
       " 'session explore',\n",
       " 'social information',\n",
       " 'social medium',\n",
       " 'social networking',\n",
       " 'social networking',\n",
       " 'social networking',\n",
       " 'spark data',\n",
       " 'specifically social',\n",
       " 'start graph',\n",
       " 'statistic graph',\n",
       " 'stochastic model',\n",
       " 'stream computing',\n",
       " 'stream computing',\n",
       " 'structure important',\n",
       " 'technique concrete',\n",
       " 'theoretical material',\n",
       " 'theory related',\n",
       " 'topic model',\n",
       " 'typical data',\n",
       " 'ubiquitous past',\n",
       " 'user data',\n",
       " 'weekly laboratory',\n",
       " 'world dedicated']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,course in enumerate(courses):\n",
    "    if course['courseId'] == 'COM-308': words_ix = words_doc[i]\n",
    "        \n",
    "words_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have created the *2-grams bag-of-words* of our corpus of the descriptions of the EPFL courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_word_index = {word: i for i,word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_word = {i:word for i,word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_doc_idx = [[dict_word_index[w] for w in doc] for doc in words_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = np.zeros(69002)\n",
    "aux[:len(aux2)] = aux2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69002"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagofwords = []\n",
    "\n",
    "for doc in words_doc_idx:\n",
    "    countwords = np.zeros(len(words))\n",
    "    countwords_bincount = np.bincount(doc)\n",
    "    countwords[:len(countwords_bincount)] = countwords_bincount\n",
    "    bagofwords_doc = {word:count for word,count in zip(words,countwords)}\n",
    "    bagofwords.append(bagofwords_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_json\n",
    "save_json(bagofwords, 'bagofwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json([dict_index_word], 'dict_index_word.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular IDF para todas las words -> mismo valor para todos los documentos (IDF siempre > 0)\n",
    "# calcular TF -> si IF == 0 -> dejamos de calcular\n",
    "# si TF > 0:\n",
    "    # calcular el TF-IDF de cada term-document\n",
    "    # ( solo añadir a la Sparse-matrix los valores > 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69002,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute **IDF** for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the max frequency of a word in the doc\n",
    "\n",
    "max_fij = []\n",
    "for doc in words_doc:\n",
    "    max_count = np.max(np.unique(doc, return_counts=True)[1])\n",
    "    max_fij.append(max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fij[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "ratio_idf = np.zeros(len(words))\n",
    "\n",
    "for j,doc in enumerate(words_doc_idx):\n",
    "    if j%100==0: print(j)\n",
    "    wordcount = np.bincount(doc)\n",
    "    for i,count in enumerate(wordcount):\n",
    "        if count > 0:\n",
    "            ratio_idf[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "pos_tf = []\n",
    "\n",
    "for j,doc in enumerate(words_doc_idx):\n",
    "    if j%100==0: print(j);\n",
    "    wordcount = np.bincount(doc)\n",
    "    for i,count in enumerate(wordcount):\n",
    "        if count > 0:\n",
    "            idf = -np.log2((ratio_idf[i])/len(words_doc_idx))\n",
    "            values.append(idf * (count / max_fij[j]))\n",
    "            pos_tf.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  528,     0],\n",
       "       [  763,     0],\n",
       "       [ 2479,     0],\n",
       "       ...,\n",
       "       [65496,   853],\n",
       "       [68051,   853],\n",
       "       [68778,   853]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tf = np.array(pos_tf)\n",
    "pos_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the sparse matrix **TF** for each word in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 295)\t1.630625951779867\n",
      "  (0, 753)\t2.717709919633111\n",
      "  (0, 818)\t4.076564879449667\n",
      "  (1, 430)\t3.24603075320683\n",
      "  (2, 97)\t9.73809225962049\n",
      "  (3, 562)\t2.4345230649051226\n",
      "  (4, 566)\t1.623015376603415\n",
      "  (5, 335)\t9.73809225962049\n",
      "  (6, 143)\t2.9126974198734965\n",
      "  (6, 146)\t2.9126974198734965\n",
      "  (7, 387)\t1.9476184519240982\n",
      "  (8, 326)\t3.24603075320683\n",
      "  (9, 61)\t2.1845230649051226\n",
      "  (9, 404)\t2.9126974198734965\n",
      "  (10, 404)\t3.24603075320683\n",
      "  (11, 61)\t2.4345230649051226\n",
      "  (12, 379)\t1.9476184519240982\n",
      "  (13, 683)\t2.9126974198734965\n",
      "  (13, 851)\t2.9126974198734965\n",
      "  (14, 113)\t2.9126974198734965\n",
      "  (14, 507)\t2.9126974198734965\n",
      "  (15, 384)\t1.9476184519240982\n",
      "  (16, 696)\t4.869046129810245\n",
      "  (17, 825)\t1.9476184519240982\n",
      "  (18, 118)\t4.369046129810245\n",
      "  :\t:\n",
      "  (68987, 807)\t4.869046129810245\n",
      "  (68988, 197)\t2.9126974198734965\n",
      "  (68988, 348)\t2.9126974198734965\n",
      "  (68989, 42)\t4.076564879449667\n",
      "  (68989, 535)\t4.076564879449667\n",
      "  (68989, 738)\t4.076564879449667\n",
      "  (68990, 362)\t2.9126974198734965\n",
      "  (68990, 393)\t2.9126974198734965\n",
      "  (68991, 572)\t1.623015376603415\n",
      "  (68992, 280)\t3.24603075320683\n",
      "  (68993, 184)\t6.49206150641366\n",
      "  (68994, 430)\t2.9126974198734965\n",
      "  (68994, 479)\t1.7476184519240983\n",
      "  (68995, 572)\t1.623015376603415\n",
      "  (68996, 309)\t2.1845230649051226\n",
      "  (68996, 766)\t1.2482988942314985\n",
      "  (68997, 37)\t1.9476184519240982\n",
      "  (68998, 37)\t1.7476184519240983\n",
      "  (68998, 766)\t1.2482988942314985\n",
      "  (68999, 309)\t2.1845230649051226\n",
      "  (68999, 766)\t1.2482988942314985\n",
      "  (69000, 309)\t2.4345230649051226\n",
      "  (69001, 42)\t4.076564879449667\n",
      "  (69001, 535)\t4.076564879449667\n",
      "  (69001, 738)\t4.076564879449667\n"
     ]
    }
   ],
   "source": [
    "m, n = len(words), len(words_doc)\n",
    "\n",
    "tf_idf = csr_matrix((values, (pos_tf[:,0],pos_tf[:,1])), shape=(m, n))\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69002, 854)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape\n",
    "\n",
    "# should not have 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('tf_idf.npz',tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = load_npz('tf_idf.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(doc1,doc2):\n",
    "    dot_doc = doc1.T @ doc2\n",
    "    norm1 = linalg.norm(doc1)\n",
    "    norm2 = linalg.norm(doc2)\n",
    "    return (dot_doc / (norm1 * norm2)).toarray()[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35957"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_word_index['markov chain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 43)\t2.3102457791876283\n",
      "  (0, 80)\t6.930737337562886\n",
      "  (0, 245)\t6.930737337562886\n",
      "  (0, 398)\t6.930737337562886\n",
      "  (0, 412)\t3.465368668781443\n",
      "  (0, 417)\t1.7326843343907214\n",
      "  (0, 555)\t1.7326843343907214\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf[35957])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([398, 245,  80, 412,  43])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_mc = np.argsort(tf_idf[35957].toarray()[0])[-5:][::-1]\n",
    "top5_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similitudes between the top 5 Markov-Chain courses are:\n",
      "\n",
      "\"Applied probability & stochastic processes\" and \"Markov chains and algorithmic applications\" is 0.0897082588764901.\n",
      "\n",
      "\"Applied probability & stochastic processes\" and \"Applied stochastic processes\" is 0.10388701735460086.\n",
      "\n",
      "\"Applied probability & stochastic processes\" and \"Optimization and simulation\" is 0.029896303540190697.\n",
      "\n",
      "\"Applied probability & stochastic processes\" and \"Internet analytics\" is 0.017162629730451507.\n",
      "\n",
      "\"Markov chains and algorithmic applications\" and \"Applied stochastic processes\" is 0.11561174226433334.\n",
      "\n",
      "\"Markov chains and algorithmic applications\" and \"Optimization and simulation\" is 0.04614616705760811.\n",
      "\n",
      "\"Markov chains and algorithmic applications\" and \"Internet analytics\" is 0.02431631585415846.\n",
      "\n",
      "\"Applied stochastic processes\" and \"Optimization and simulation\" is 0.050484747799964265.\n",
      "\n",
      "\"Applied stochastic processes\" and \"Internet analytics\" is 0.021609791757521223.\n",
      "\n",
      "\"Optimization and simulation\" and \"Internet analytics\" is 0.005059741323293343.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The similitudes between the top 5 Markov-Chain courses are:')\n",
    "print()\n",
    "\n",
    "for i in range(len(top5_mc)):\n",
    "    doc1 = tf_idf[:,top5_mc[i]] \n",
    "    course1 = courses[top5_mc[i]]['name']\n",
    "    \n",
    "    for j in range(i+1,len(top5_mc)):\n",
    "        doc2 = tf_idf[:,top5_mc[j]] \n",
    "        course2 = courses[top5_mc[j]]['name']\n",
    "        \n",
    "        sim_docs = sim(doc1,doc2)\n",
    "#         print(type(sim_docs))\n",
    "        print(f'\"{course1}\" and \"{course2}\" is {sim_docs}.')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ab initio', 'ab power', 'ab variable', ..., 'énergétique master',\n",
       "       'énergétique notion', 'être pris'], dtype='<U75')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_idxs = [dict_word_index[w] for w in words \n",
    "                   if 'facebook' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22303, 22304, 22305, 54060, 57412, 66176]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 798)\t0.749084019970807\n",
      "  (1, 798)\t0.749084019970807\n",
      "  (2, 798)\t0.749084019970807\n",
      "  (3, 798)\t0.749084019970807\n",
      "  (4, 798)\t0.749084019970807\n",
      "  (5, 798)\t0.749084019970807\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf[facebook_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similitudes between the top 5 Markov-Chain and the only course that contains Facebook are:\n",
      "\n",
      "\"Applied probability & stochastic processes\" and \"Computational Social Media\" is 0.0.\n",
      "\n",
      "\"Markov chains and algorithmic applications\" and \"Computational Social Media\" is 0.008236951339467167.\n",
      "\n",
      "\"Applied stochastic processes\" and \"Computational Social Media\" is 0.0.\n",
      "\n",
      "\"Optimization and simulation\" and \"Computational Social Media\" is 0.0.\n",
      "\n",
      "\"Internet analytics\" and \"Computational Social Media\" is 0.040662506491494395.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The similitudes between the top 5 Markov-Chain and the only course that contains Facebook are:')\n",
    "print()\n",
    "\n",
    "doc_face = tf_idf[:,798] \n",
    "course_face = courses[798]['name']\n",
    "\n",
    "for i in range(len(top5_mc)):\n",
    "    doc1 = tf_idf[:,top5_mc[i]] \n",
    "    course1 = courses[top5_mc[i]]['name']\n",
    "    sim_docs = sim(doc1,doc_face)\n",
    "\n",
    "    print(f'\"{course1}\" and \"{course_face}\" is {sim_docs}.')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
