{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "[Spark](https://spark.apache.org/docs/2.3.2/index.html) (we are going to use version 2.3.2) is a fast and general-purpose cluster computing system, that allows to process large datasets using several machines. In the era of the *big data*, it has become a necessity to be able to distribute computing power and process data in parallel. We introduce here the basics of the framework.\n",
    "\n",
    "In any case, you should have a look at the [official tutorial](https://spark.apache.org/docs/2.3.2/programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop File System (HDFS)\n",
    "\n",
    "The main abstraction Spark offers is the *Resisilient Distributed Dataset* (RDD), which is a collection of elements distributed across the nodes of the cluster. The idea is that you typically **do not** load them in the memory, but rather process them *lazily*. They are initialized with files in the *Hadoop File System* (HDFS). \n",
    "\n",
    "Let us first see what there is in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Found 2 items\n",
      "drwxr-xr-x   - prunonos hadoop          0 2022-03-01 18:34 .sparkStaging\n",
      "-rw-r--r--   3 prunonos hadoop    2147813 2022-02-23 12:07 election-day-tweets.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* a line starting with `!` indicates a shell command. You can hence run this same command in a bash terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add a dataset to HFDS. In the `data` folder, there is text file containing 30000 tweets collected from the [Inauguration Day](https://en.wikipedia.org/wiki/United_States_presidential_inauguration) on January 20, 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ../data/election-day-tweets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that the file is indeed in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Found 2 items\n",
      "drwxr-xr-x   - prunonos hadoop          0 2022-02-23 11:46 .sparkStaging\n",
      "-rw-r--r--   3 prunonos hadoop    2147813 2022-02-23 12:07 election-day-tweets.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* you can remove files from HDFS using `hdfs dfs -rm FILE`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset processing\n",
    "\n",
    "We can now start playing with the tweets. We will instantiate the dataset from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sc.textFile('election-day-tweets.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tweets` variable is now an RDD. An RDD is *resilient* to faults by storing the sequence of operations applied to it so that it can easily recreate the state of the data structure in case of a problem. It can also be logically cut to be *distributed* on several machines.\n",
    "\n",
    "The `sc` object is a special object, namely a [Spark Context](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext) obeject, that we configured to be available everywhere in your Python environment. It is the main interface with Spark that has numerous useful methods. Check the documentation!\n",
    "\n",
    "Let us display a few tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: @Lawrence @HillaryClinton Two first  @SenSchumer tomorrow. @TheLastWord #brooklyn  TheRealAmerica #Vote #Democrats #NastyWomenVote #Senate\n",
      "1: My @latimesopinion op-ed on historic #California #Senate race. First time an elected woman senator succeeds another.\n",
      "2: https://t.co/cbjQTK0Q1V\n",
      "3: #Senate Wisconsin Senate Preview: Johnson vs. Feingold\n",
      "4: If Rubio Wins and #Trump Loses in #Florida... #HillaryClinton #Senate #RepublicanPrimary #Senaterace #Miami... https://t.co/zIeNEcVnMO\n",
      "5: #Senate Wisconsin Senate Preview: Johnson vs. Feingold\n",
      "6: bob day is an honest  person   #senate patterson . a loss to the senate\n",
      "7: Make Republicans #PayAPrice!\n",
      "8:  üíôüá∫üá∏#VoteBLUEüîÉtheBallotüá∫üá∏üíô\n",
      "9:  #Congress #Senate #FlipItDem\n"
     ]
    }
   ],
   "source": [
    "some_tweets = tweets.take(10)\n",
    "for i, tweet in enumerate(some_tweets):\n",
    "    print(\"%d: %s\" % (i, tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the `tweets` dataset can be acted on by dataset **operations**. RDDs support two types of operations: *transformations*, which create a new RDD from an existing one, and *actions*, which return a value to the driver program after running a computation on the RDD. For instance, we can `count` the number of tweets. Is it a transofrmation or an action? _action_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also `filter` the lines matching a specific string. Is it a transformation or an action ?\n",
    "\n",
    "*Note*: `lower()` is a `str` method that converts the string to lower case. What happens if you do not do that? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_trump = tweets.filter(lambda line: \"trump\" in line.lower())\n",
    "lines_with_trump.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical processing pattern is [MapReduce](https://en.wikipedia.org/wiki/MapReduce), as popularized by Hadoop. The idea is that you apply a function to your dataset that filters or sorts it (**map**), and then combine the ouputs by computing something (**reduce**). The typical example is a word count. So, how many times each word occur among our tweets?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lawrence',\n",
       " '@HillaryClinton',\n",
       " 'Two',\n",
       " 'first',\n",
       " '@SenSchumer',\n",
       " 'tomorrow.',\n",
       " '@TheLastWord',\n",
       " '#brooklyn',\n",
       " 'TheRealAmerica',\n",
       " '#Vote',\n",
       " '#Democrats',\n",
       " '#NastyWomenVote',\n",
       " '#Senate',\n",
       " 'My',\n",
       " '@latimesopinion']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split every line into words and flattens the result (aggregate them all together)\n",
    "words = tweets.flatMap(lambda line: line.split()) \n",
    "# si usaramos map(func) a secas devolveria una lista de listas\n",
    "words.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@Lawrence', 1),\n",
       " ('@HillaryClinton', 1),\n",
       " ('Two', 1),\n",
       " ('first', 1),\n",
       " ('@SenSchumer', 1),\n",
       " ('tomorrow.', 1),\n",
       " ('@TheLastWord', 1),\n",
       " ('#brooklyn', 1),\n",
       " ('TheRealAmerica', 1),\n",
       " ('#Vote', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign 1 to each word\n",
    "counts = words.map(lambda word: (word, 1))\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@mymy__Mimi', 1),\n",
       " ('say', 195),\n",
       " ('more', 537),\n",
       " ('in', 3794),\n",
       " ('üëèüèæüëèüèæüëèüèæ', 2),\n",
       " ('https://t.co/Txza5n7DIM', 3),\n",
       " ('of', 4654),\n",
       " (\"y'all\", 87),\n",
       " ('congress', 3675),\n",
       " ('we', 677)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combines the counts by adding up the value (1) of equal keys (words)\n",
    "word_counts = counts.reduceByKey(lambda a, b: a + b)\n",
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read the list of common [transformations](https://spark.apache.org/docs/1.6.2/programming-guide.html#transformations) and [actions](https://spark.apache.org/docs/1.6.2/programming-guide.html#actions) to get an idea of what you can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics\n",
    "\n",
    "Your turn! Try to extract some statisctis from the dataset:\n",
    "\n",
    "1. Total number of words\n",
    "2. Average number of words per tweet\n",
    "3. Ten most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300882"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "# 1. Total number of words\n",
    "total_words = tweets.flatMap(lambda line: line.split()).count()\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0294"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Average number of words per tweet\n",
    "total_words / tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Congress', 8526),\n",
       " ('the', 6494),\n",
       " ('to', 5829),\n",
       " ('of', 4654),\n",
       " ('for', 3964),\n",
       " ('in', 3794),\n",
       " ('congress', 3675),\n",
       " ('is', 3478),\n",
       " ('and', 3295),\n",
       " ('a', 2985)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Ten most frequent word\n",
    "\n",
    "word_counts.takeOrdered(10,key=lambda v: -v[1])\n",
    "# word_counts.takeOrdered(10,key=lambda x: -tuple(x)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "A bigram is combination of two consecutive words. \n",
    "\n",
    "- Extract the ten most frequent bigrams from the dataset. \n",
    "\n",
    "Be careful not to consider the last word of a tweet and the first one of the next tweet!\n",
    "\n",
    "*Hint:* note that the `map` and `flatMap` methods take a **function** as argument, meaning that you can define a function that you pass as argument:\n",
    "\n",
    "```python\n",
    "def process(line):\n",
    "    # Do something\n",
    "    return something\n",
    "\n",
    "x = tweets.flatMap(process)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of Congress', 677),\n",
       " ('of the', 542),\n",
       " ('Congress is', 518),\n",
       " ('in the', 429),\n",
       " ('in Congress', 410),\n",
       " ('for Congress', 366),\n",
       " ('vote for', 357),\n",
       " ('up for', 344),\n",
       " ('can do', 305),\n",
       " ('for grabs', 293)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "def process_twt(tweet):\n",
    "    bigrams = [(tweet[i]+' '+tweet[i+1],1) for i in range(len(tweet)-1)]\n",
    "    return bigrams\n",
    "\n",
    "tweets_splitted = tweets.map(lambda t: t.split())\n",
    "bigrams = tweets_splitted.map(process_twt)\n",
    "bigram_flat = bigrams.flatMap(lambda x:x)\n",
    "bigram_ordered = (bigram_flat.reduceByKey(lambda a,b: a + b)).takeOrdered(10,key=lambda v: -v[1])\n",
    "bigram_ordered"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
